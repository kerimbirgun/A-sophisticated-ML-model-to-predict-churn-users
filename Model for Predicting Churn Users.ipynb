{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "import plotly.figure_factory as ff\n",
    "import warnings\n",
    "import os\n",
    "import plotly.express as px\n",
    "import xgboost as xgb\n",
    "from datetime import timedelta\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datasets\n",
    "demographics = pd.read_csv('demographics.csv')\n",
    "purchases = pd.read_csv('purchases.csv')\n",
    "variable_mapping = pd.read_csv('variable_mapping.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MISSING VALUE IMPUTATION FOR PURCHASES DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if these is any null values in the purchases dataset\n",
    "print(purchases.isnull().sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Missing Value Imputation for Units feature - using 10 iteration\n",
    "list1 = ['UNITS']\n",
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp.fit(purchases[list1])\n",
    "IterativeImputer(random_state=0)\n",
    "# Transform the dataset based on fitted units feature\n",
    "purchases[list1] = imp.transform(purchases[list1])\n",
    "# Check the operation\n",
    "print(purchases.isnull().sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K Generating Year-Month-Day columns for further analysis\n",
    "purchases['TRANSACTION_DT'] = pd.to_datetime(purchases['TRANSACTION_DT'])\n",
    "purchases['Year'] = purchases['TRANSACTION_DT'].dt.year\n",
    "purchases['Month'] = purchases['TRANSACTION_DT'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTIVE MODEL FOR CHURN USERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the year and month column\n",
    "purchases[\"Year_Month\"]=purchases[\"Year\"].astype(str)+\"-\"+purchases[\"Month\"].astype(str)\n",
    "\n",
    "#Set up the month order\n",
    "month_order = [\"2018-1\",\"2018-2\",\"2018-3\",\"2018-4\",\"2018-5\",\"2018-6\",\"2018-7\",\"2018-8\",\n",
    "               \"2018-9\",\"2018-10\",\"2018-11\",\"2018-12\",\"2019-1\",\"2019-2\",\"2019-3\",\"2019-4\",\n",
    "               \"2019-5\",\"2019-6\",\"2019-7\",\"2019-8\",\"2019-9\",\"2019-10\",\"2019-11\",\"2019-12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the data for time series analysis\n",
    "ts = purchases.groupby('USER_ID')['Year_Month'].value_counts().unstack().fillna(0)[month_order]\n",
    "# Reset index to utilize 'USER_ID' feature\n",
    "ts = ts.reset_index()\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 11 identical year blocks based on 11 different months (the aim is to distill as much 'churn' user data as possible for the model)\n",
    "mo_1 = purchases[(purchases['TRANSACTION_DT'] > '2018-02-01') & (purchases['TRANSACTION_DT'] < '2019-02-01')]\n",
    "mo_2 = purchases[(purchases['TRANSACTION_DT'] > '2018-03-01') & (purchases['TRANSACTION_DT'] < '2019-03-01')]\n",
    "mo_3 = purchases[(purchases['TRANSACTION_DT'] > '2018-04-01') & (purchases['TRANSACTION_DT'] < '2019-04-01')]\n",
    "mo_4 = purchases[(purchases['TRANSACTION_DT'] > '2018-05-01') & (purchases['TRANSACTION_DT'] < '2019-05-01')]\n",
    "mo_5 = purchases[(purchases['TRANSACTION_DT'] > '2018-06-01') & (purchases['TRANSACTION_DT'] < '2019-06-01')]\n",
    "mo_6 = purchases[(purchases['TRANSACTION_DT'] > '2018-07-01') & (purchases['TRANSACTION_DT'] < '2019-07-01')]\n",
    "mo_7 = purchases[(purchases['TRANSACTION_DT'] > '2018-08-01') & (purchases['TRANSACTION_DT'] < '2019-08-01')]\n",
    "mo_8 = purchases[(purchases['TRANSACTION_DT'] > '2018-09-01') & (purchases['TRANSACTION_DT'] < '2019-09-01')]\n",
    "mo_9 = purchases[(purchases['TRANSACTION_DT'] > '2018-10-01') & (purchases['TRANSACTION_DT'] < '2019-10-01')]\n",
    "mo_10 = purchases[(purchases['TRANSACTION_DT'] > '2018-11-01') & (purchases['TRANSACTION_DT'] < '2019-11-01')]\n",
    "mo_11 = purchases[(purchases['TRANSACTION_DT'] > '2018-12-01') & (purchases['TRANSACTION_DT'] < '2019-12-01')]\n",
    "len(mo_1.groupby('TRANSACTION_DT')['TRANSACTION_DT']) # just to check a random block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Recency and Frequency Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Recency and Frequency features for the 1st block\n",
    "snapshot_date = mo_1['TRANSACTION_DT'].max() + timedelta(days=1)\n",
    "mo_1 = mo_1.groupby(['USER_ID']).agg({'TRANSACTION_DT': lambda x: (snapshot_date - x.max()).days,'BASKET_ID': 'count'})\n",
    "mo_1.rename(columns={'TRANSACTION_DT': 'Recency','BASKET_ID': 'Frequency'}, inplace=True)\n",
    "# enerate Recency and Frequency features for the 2nd block\n",
    "snapshot_date = mo_2['TRANSACTION_DT'].max() + timedelta(days=1)\n",
    "mo_2 = mo_2.groupby(['USER_ID']).agg({'TRANSACTION_DT': lambda x: (snapshot_date - x.max()).days,'BASKET_ID': 'count'})\n",
    "mo_2.rename(columns={'TRANSACTION_DT': 'Recency','BASKET_ID': 'Frequency'}, inplace=True)\n",
    "# enerate Recency and Frequency features for the 3rd block\n",
    "snapshot_date = mo_3['TRANSACTION_DT'].max() + timedelta(days=1)\n",
    "mo_3 = mo_3.groupby(['USER_ID']).agg({'TRANSACTION_DT': lambda x: (snapshot_date - x.max()).days,'BASKET_ID': 'count'})\n",
    "mo_3.rename(columns={'TRANSACTION_DT': 'Recency','BASKET_ID': 'Frequency'}, inplace=True)\n",
    "# enerate Recency and Frequency features for the 4th block\n",
    "snapshot_date = mo_4['TRANSACTION_DT'].max() + timedelta(days=1)\n",
    "mo_4 = mo_4.groupby(['USER_ID']).agg({'TRANSACTION_DT': lambda x: (snapshot_date - x.max()).days,'BASKET_ID': 'count'})\n",
    "mo_4.rename(columns={'TRANSACTION_DT': 'Recency','BASKET_ID': 'Frequency'}, inplace=True)\n",
    "# enerate Recency and Frequency features for the 5th block\n",
    "snapshot_date = mo_5['TRANSACTION_DT'].max() + timedelta(days=1)\n",
    "mo_5 = mo_5.groupby(['USER_ID']).agg({'TRANSACTION_DT': lambda x: (snapshot_date - x.max()).days,'BASKET_ID': 'count'})\n",
    "mo_5.rename(columns={'TRANSACTION_DT': 'Recency','BASKET_ID': 'Frequency'}, inplace=True)\n",
    "# enerate Recency and Frequency features for the 6th block\n",
    "snapshot_date = mo_6['TRANSACTION_DT'].max() + timedelta(days=1)\n",
    "mo_6 = mo_6.groupby(['USER_ID']).agg({'TRANSACTION_DT': lambda x: (snapshot_date - x.max()).days,'BASKET_ID': 'count'})\n",
    "mo_6.rename(columns={'TRANSACTION_DT': 'Recency','BASKET_ID': 'Frequency'}, inplace=True)\n",
    "# enerate Recency and Frequency features for the 7th block\n",
    "snapshot_date = mo_7['TRANSACTION_DT'].max() + timedelta(days=1)\n",
    "mo_7 = mo_7.groupby(['USER_ID']).agg({'TRANSACTION_DT': lambda x: (snapshot_date - x.max()).days,'BASKET_ID': 'count'})\n",
    "mo_7.rename(columns={'TRANSACTION_DT': 'Recency','BASKET_ID': 'Frequency'}, inplace=True)\n",
    "# enerate Recency and Frequency features for the 8th block\n",
    "snapshot_date = mo_8['TRANSACTION_DT'].max() + timedelta(days=1)\n",
    "mo_8 = mo_8.groupby(['USER_ID']).agg({'TRANSACTION_DT': lambda x: (snapshot_date - x.max()).days,'BASKET_ID': 'count'})\n",
    "mo_8.rename(columns={'TRANSACTION_DT': 'Recency','BASKET_ID': 'Frequency'}, inplace=True)\n",
    "# enerate Recency and Frequency features for the 9th block\n",
    "snapshot_date = mo_9['TRANSACTION_DT'].max() + timedelta(days=1)\n",
    "mo_9 = mo_9.groupby(['USER_ID']).agg({'TRANSACTION_DT': lambda x: (snapshot_date - x.max()).days,'BASKET_ID': 'count'})\n",
    "mo_9.rename(columns={'TRANSACTION_DT': 'Recency','BASKET_ID': 'Frequency'}, inplace=True)\n",
    "# enerate Recency and Frequency features for the 10th block\n",
    "snapshot_date = mo_10['TRANSACTION_DT'].max() + timedelta(days=1)\n",
    "mo_10 = mo_10.groupby(['USER_ID']).agg({'TRANSACTION_DT': lambda x: (snapshot_date - x.max()).days,'BASKET_ID': 'count'})\n",
    "mo_10.rename(columns={'TRANSACTION_DT': 'Recency','BASKET_ID': 'Frequency'}, inplace=True)\n",
    "# enerate Recency and Frequency features for the 11th block\n",
    "snapshot_date = mo_11['TRANSACTION_DT'].max() + timedelta(days=1)\n",
    "mo_11 = mo_11.groupby(['USER_ID']).agg({'TRANSACTION_DT': lambda x: (snapshot_date - x.max()).days,'BASKET_ID': 'count'})\n",
    "mo_11.rename(columns={'TRANSACTION_DT': 'Recency','BASKET_ID': 'Frequency'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_11.head(3) # just to check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Data Preparation for each Month - making them identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read prevıously generated ts_data as ts\n",
    "ts = pd.read_csv('ts_data.csv')\n",
    "# Create a derivative dataset solely composed of months\n",
    "ts2 = ts.iloc[:,1:]\n",
    "ts2[ts2['2019-5'] == 0].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1st yearly block starting with 2018-2 \n",
    "month1 = ts2.iloc[:,np.r_[0,2:15]]\n",
    "# Integrate User_ID features Recency and Frequency\n",
    "month1 = month1.merge(mo_1,left_on='USER_ID', right_on='USER_ID') \n",
    "month1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd yearly block starting with 2018-3 \n",
    "month2 = ts2.iloc[:,np.r_[0,3:16]]\n",
    "# Integrate User_ID features Recency and Frequency\n",
    "month2 = month2.merge(mo_2,left_on='USER_ID', right_on='USER_ID') \n",
    "month2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd yearly block starting with 2018-4 \n",
    "month3 = ts2.iloc[:,np.r_[0,4:17]]\n",
    "# Integrate User_ID features Recency and Frequency\n",
    "month3 = month3.merge(mo_3,left_on='USER_ID', right_on='USER_ID') \n",
    "month3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4th yearly block starting with 2018-5 \n",
    "month4 = ts2.iloc[:,np.r_[0,5:18]]\n",
    "# Integrate User_ID features Recency and Frequency\n",
    "month4 = month4.merge(mo_4,left_on='USER_ID', right_on='USER_ID') \n",
    "month4.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5th yearly block starting with 2018-6 \n",
    "month5 = ts2.iloc[:,np.r_[0,6:19]]\n",
    "# Integrate User_ID features Recency and Frequency\n",
    "month5 = month5.merge(mo_5,left_on='USER_ID', right_on='USER_ID') \n",
    "month5.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6th yearly block starting with 2018-7 \n",
    "month6 = ts2.iloc[:,np.r_[0,7:20]]\n",
    "# Integrate User_ID features Recency and Frequency\n",
    "month6 = month6.merge(mo_6,left_on='USER_ID', right_on='USER_ID') \n",
    "month6.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7th yearly block starting with 2018-8 \n",
    "month7 = ts2.iloc[:,np.r_[0,8:21]]\n",
    "# Integrate User_ID features Recency and Frequency\n",
    "month7 = month7.merge(mo_7,left_on='USER_ID', right_on='USER_ID') \n",
    "month7.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8th yearly block starting with 2018-9 \n",
    "month8 = ts2.iloc[:,np.r_[0,9:22]]\n",
    "# Integrate User_ID features Recency and Frequency\n",
    "month8 = month8.merge(mo_8,left_on='USER_ID', right_on='USER_ID') \n",
    "month8.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9th yearly block starting with 2018-10 \n",
    "month9 = ts2.iloc[:,np.r_[0,10:23]]\n",
    "# Integrate User_ID features Recency and Frequency\n",
    "month9 = month9.merge(mo_9,left_on='USER_ID', right_on='USER_ID') \n",
    "month9.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10th yearly block starting with 2018-11 \n",
    "month10 = ts2.iloc[:,np.r_[0,11:24]]\n",
    "# Integrate User_ID features Recency and Frequency\n",
    "month10 = month10.merge(mo_10,left_on='USER_ID', right_on='USER_ID') \n",
    "month10.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11th yearly block starting with 2018-12 \n",
    "month11 = ts2.iloc[:,np.r_[0,12:25]]\n",
    "# Integrate User_ID features Recency and Frequency\n",
    "month11 = month11.merge(mo_11,left_on='USER_ID', right_on='USER_ID') \n",
    "month11.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete previously created dummy datasets\n",
    "del mo_1;del mo_2; del mo_3; del mo_4; del mo_5; del mo_6; del mo_7; del mo_8; del mo_9; del mo_10; del mo_11  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Active-Churn-New Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a for loop to define User Status for the year block  - month1 \n",
    "User_Status = []\n",
    "month = month1\n",
    "rows = len(month)\n",
    "# Define formulas based on Active, Churn customer definitions of Company X\n",
    "for x in range(0,rows):\n",
    "    if  (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] > 0):\n",
    "        val = 1 # ACTIVE USER    \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] == 0):\n",
    "        val = 0  # JUST CHURNED   \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] == 0):\n",
    "        val = 2  # ALREADY CHURNED (DROP FROM THE MODEL BEFOREHAND)\n",
    "    elif (month.iloc[x,12] == 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0):\n",
    "        val = 2  # ALREADY CHURNED  (DROP FROM THE MODEL BEFOREHAND)\n",
    "    elif (month.iloc[x,12] > 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0)&(month.iloc[x,9] == 0):\n",
    "        val = 3  # NEW CUSTOMER (DON'T INVOLVE INTO MODEL -DROP- )\n",
    "    else:\n",
    "        val = 1 # REST COULD BE CONSIDERED ACTIVE\n",
    "    User_Status.append(val)\n",
    "User_Status = pd.DataFrame(User_Status,columns = {\"User_Status\"})\n",
    "# Integrate user definitions with pre generated time series dataset \n",
    "ts3_1 = pd.concat([month1,User_Status],axis=1)\n",
    "print(ts3_1.groupby('User_Status')['2019-2'].count())\n",
    "################################################################################################################\n",
    "# Create a for loop to define User Status for the year block  - month2\n",
    "User_Status = []\n",
    "month = month2\n",
    "rows = len(month)\n",
    "# Define formulas based on Active, Churn customer definitions of Company X\n",
    "for x in range(0,rows):\n",
    "    if  (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] > 0):\n",
    "        val = 1 # ACTIVE USER    \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] == 0):\n",
    "        val = 0  # JUST CHURNED   \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] == 0):\n",
    "        val = 2  # ALREADY CHURNED \n",
    "    elif (month.iloc[x,12] == 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0):\n",
    "        val = 2  # ALREADY CHURNED\n",
    "    elif (month.iloc[x,12] > 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0)&(month.iloc[x,9] == 0):\n",
    "        val = 3  # NEW CUSTOMER (DON'T INVOLVE INTO MODEL -DROP- )\n",
    "    else:\n",
    "        val = 1 # REST COULD BE CONSIDERED ACTIVE\n",
    "    User_Status.append(val)\n",
    "User_Status = pd.DataFrame(User_Status,columns = {\"User_Status\"})\n",
    "# Integrate user definitions with pre generated time series dataset \n",
    "ts3_2 = pd.concat([month2,User_Status],axis=1)\n",
    "print(ts3_2.groupby('User_Status')['2019-3'].count())\n",
    "#################################################################################################################\n",
    "# Create a for loop to define User Status for the year block  - month3 \n",
    "User_Status = []\n",
    "month = month3\n",
    "rows = len(month)\n",
    "# Define formulas based on Active, Churn customer definitions of Company X\n",
    "for x in range(0,rows):\n",
    "    if  (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] > 0):\n",
    "        val = 1 # ACTIVE USER    \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] == 0):\n",
    "        val = 0  # JUST CHURNED   \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] == 0):\n",
    "        val = 2  # ALREADY CHURNED \n",
    "    elif (month.iloc[x,12] == 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0):\n",
    "        val = 2  # ALREADY CHURNED\n",
    "    elif (month.iloc[x,12] > 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0)&(month.iloc[x,9] == 0):\n",
    "        val = 3  # NEW CUSTOMER (DON'T INVOLVE INTO MODEL -DROP- )\n",
    "    else:\n",
    "        val = 1 # REST COULD BE CONSIDERED ACTIVE\n",
    "    User_Status.append(val)\n",
    "User_Status = pd.DataFrame(User_Status,columns = {\"User_Status\"})\n",
    "# Integrate user definitions with pre generated time series dataset \n",
    "ts3_3 = pd.concat([month3,User_Status],axis=1)\n",
    "print(ts3_3.groupby('User_Status')['2019-4'].count())\n",
    "#################################################################################################################\n",
    "# Create a for loop to define User Status for the year block  - month4 \n",
    "User_Status = []\n",
    "month = month4\n",
    "rows = len(month)\n",
    "# Define formulas based on Active, Churn customer definitions of Company X\n",
    "for x in range(0,rows):\n",
    "    if  (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] > 0):\n",
    "        val = 1 # ACTIVE USER    \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] == 0):\n",
    "        val = 0  # JUST CHURNED   \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] == 0):\n",
    "        val = 2  # ALREADY CHURNED \n",
    "    elif (month.iloc[x,12] == 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0):\n",
    "        val = 2  # ALREADY CHURNED\n",
    "    elif (month.iloc[x,12] > 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0)&(month.iloc[x,9] == 0):\n",
    "        val = 3  # NEW CUSTOMER (DON'T INVOLVE INTO MODEL -DROP- )\n",
    "    else:\n",
    "        val = 1 # REST COULD BE CONSIDERED ACTIVE\n",
    "    User_Status.append(val)\n",
    "User_Status = pd.DataFrame(User_Status,columns = {\"User_Status\"})\n",
    "# Integrate user definitions with pre generated time series dataset \n",
    "ts3_4 = pd.concat([month4,User_Status],axis=1)\n",
    "print(ts3_4.groupby('User_Status')['2019-5'].count())\n",
    "#################################################################################################################\n",
    "# Create a for loop to define User Status for the year block  - month5 \n",
    "User_Status = []\n",
    "month = month5\n",
    "rows = len(month)\n",
    "# Define formulas based on Active, Churn customer definitions of Company X\n",
    "for x in range(0,rows):\n",
    "    if  (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] > 0):\n",
    "        val = 1 # ACTIVE USER    \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] == 0):\n",
    "        val = 0  # JUST CHURNED   \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] == 0):\n",
    "        val = 2  # ALREADY CHURNED \n",
    "    elif (month.iloc[x,12] == 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0):\n",
    "        val = 2  # ALREADY CHURNED\n",
    "    elif (month.iloc[x,12] > 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0)&(month.iloc[x,9] == 0):\n",
    "        val = 3  # NEW CUSTOMER (DON'T INVOLVE INTO MODEL -DROP- )\n",
    "    else:\n",
    "        val = 1 # REST COULD BE CONSIDERED ACTIVE\n",
    "    User_Status.append(val)\n",
    "User_Status = pd.DataFrame(User_Status,columns = {\"User_Status\"})\n",
    "# Integrate user definitions with pre generated time series dataset \n",
    "ts3_5 = pd.concat([month5,User_Status],axis=1)\n",
    "print(ts3_5.groupby('User_Status')['2019-6'].count())\n",
    "#################################################################################################################\n",
    "# Create a for loop to define User Status for the year block  - month6 \n",
    "User_Status = []\n",
    "month = month6\n",
    "rows = len(month)\n",
    "# Define formulas based on Active, Churn customer definitions of Company X\n",
    "for x in range(0,rows):\n",
    "    if  (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] > 0):\n",
    "        val = 1 # ACTIVE USER    \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] == 0):\n",
    "        val = 0  # JUST CHURNED   \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] == 0):\n",
    "        val = 2  # ALREADY CHURNED \n",
    "    elif (month.iloc[x,12] == 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0):\n",
    "        val = 2  # ALREADY CHURNED\n",
    "    elif (month.iloc[x,12] > 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0)&(month.iloc[x,9] == 0):\n",
    "        val = 3  # NEW CUSTOMER (DON'T INVOLVE INTO MODEL -DROP- )\n",
    "    else:\n",
    "        val = 1 # REST COULD BE CONSIDERED ACTIVE\n",
    "    User_Status.append(val)\n",
    "User_Status = pd.DataFrame(User_Status,columns = {\"User_Status\"})\n",
    "# Integrate user definitions with pre generated time series dataset \n",
    "ts3_6 = pd.concat([month6,User_Status],axis=1)\n",
    "print(ts3_6.groupby('User_Status')['2019-7'].count())\n",
    "#################################################################################################################\n",
    "# Create a for loop to define User Status for the year block  - month7 \n",
    "User_Status = []\n",
    "month = month7\n",
    "rows = len(month)\n",
    "# Define formulas based on Active, Churn customer definitions of Company X\n",
    "for x in range(0,rows):\n",
    "    if  (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] > 0):\n",
    "        val = 1 # ACTIVE USER    \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] == 0):\n",
    "        val = 0  # JUST CHURNED   \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] == 0):\n",
    "        val = 2  # ALREADY CHURNED \n",
    "    elif (month.iloc[x,12] == 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0):\n",
    "        val = 2  # ALREADY CHURNED\n",
    "    elif (month.iloc[x,12] > 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0)&(month.iloc[x,9] == 0):\n",
    "        val = 3  # NEW CUSTOMER (DON'T INVOLVE INTO MODEL -DROP- )\n",
    "    else:\n",
    "        val = 1 # REST COULD BE CONSIDERED ACTIVE\n",
    "    User_Status.append(val)\n",
    "User_Status = pd.DataFrame(User_Status,columns = {\"User_Status\"})\n",
    "# Integrate user definitions with pre generated time series dataset \n",
    "ts3_7 = pd.concat([month7,User_Status],axis=1)\n",
    "print(ts3_7.groupby('User_Status')['2019-8'].count())\n",
    "#################################################################################################################\n",
    "# Create a for loop to define User Status for the year block  - month8 \n",
    "User_Status = []\n",
    "month = month8\n",
    "rows = len(month)\n",
    "# Define formulas based on Active, Churn customer definitions of Company X\n",
    "for x in range(0,rows):\n",
    "    if  (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] > 0):\n",
    "        val = 1 # ACTIVE USER    \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] == 0):\n",
    "        val = 0  # JUST CHURNED   \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] == 0):\n",
    "        val = 2  # ALREADY CHURNED \n",
    "    elif (month.iloc[x,12] == 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0):\n",
    "        val = 2  # ALREADY CHURNED\n",
    "    elif (month.iloc[x,12] > 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0)&(month.iloc[x,9] == 0):\n",
    "        val = 3  # NEW CUSTOMER (DON'T INVOLVE INTO MODEL -DROP- )\n",
    "    else:\n",
    "        val = 1 # REST COULD BE CONSIDERED ACTIVE\n",
    "    User_Status.append(val)\n",
    "User_Status = pd.DataFrame(User_Status,columns = {\"User_Status\"})\n",
    "# Integrate user definitions with pre generated time series dataset \n",
    "ts3_8 = pd.concat([month8,User_Status],axis=1)\n",
    "print(ts3_8.groupby('User_Status')['2019-9'].count())\n",
    "#################################################################################################################\n",
    "# Create a for loop to define User Status for the year block  - month9 \n",
    "User_Status = []\n",
    "month = month9\n",
    "rows = len(month)\n",
    "# Define formulas based on Active, Churn customer definitions of Company X\n",
    "for x in range(0,rows):\n",
    "    if  (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] > 0):\n",
    "        val = 1 # ACTIVE USER    \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] == 0):\n",
    "        val = 0  # JUST CHURNED   \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] == 0):\n",
    "        val = 2  # ALREADY CHURNED \n",
    "    elif (month.iloc[x,12] == 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0):\n",
    "        val = 2  # ALREADY CHURNED\n",
    "    elif (month.iloc[x,12] > 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0)&(month.iloc[x,9] == 0):\n",
    "        val = 3  # NEW CUSTOMER (DON'T INVOLVE INTO MODEL -DROP- )\n",
    "    else:\n",
    "        val = 1 # REST COULD BE CONSIDERED ACTIVE\n",
    "    User_Status.append(val)\n",
    "User_Status = pd.DataFrame(User_Status,columns = {\"User_Status\"})\n",
    "# Integrate user definitions with pre generated time series dataset \n",
    "ts3_9 = pd.concat([month9,User_Status],axis=1)\n",
    "print(ts3_9.groupby('User_Status')['2019-10'].count())\n",
    "#################################################################################################################\n",
    "# Create a for loop to define User Status for the year block  - month10 \n",
    "User_Status = []\n",
    "month = month10\n",
    "rows = len(month)\n",
    "# Define formulas based on Active, Churn customer definitions of Company X\n",
    "for x in range(0,rows):\n",
    "    if  (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] > 0):\n",
    "        val = 1 # ACTIVE USER    \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] == 0):\n",
    "        val = 0  # JUST CHURNED   \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] == 0):\n",
    "        val = 2  # ALREADY CHURNED \n",
    "    elif (month.iloc[x,12] == 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0):\n",
    "        val = 2  # ALREADY CHURNED\n",
    "    elif (month.iloc[x,12] > 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0)&(month.iloc[x,9] == 0):\n",
    "        val = 3  # NEW CUSTOMER (DON'T INVOLVE INTO MODEL -DROP- )\n",
    "    else:\n",
    "        val = 1 # REST COULD BE CONSIDERED ACTIVE\n",
    "    User_Status.append(val)\n",
    "User_Status = pd.DataFrame(User_Status,columns = {\"User_Status\"})\n",
    "# Integrate user definitions with pre generated time series dataset \n",
    "ts3_10 = pd.concat([month10,User_Status],axis=1)\n",
    "print(ts3_10.groupby('User_Status')['2019-11'].count())\n",
    "#################################################################################################################\n",
    "# Create a for loop to define User Status for the year block  - month11 \n",
    "User_Status = []\n",
    "month = month11\n",
    "rows = len(month)\n",
    "# Define formulas based on Active, Churn customer definitions of Company X\n",
    "for x in range(0,rows):\n",
    "    if  (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] > 0):\n",
    "        val = 1 # ACTIVE USER    \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] > 0)&(month.iloc[x,13] == 0):\n",
    "        val = 0  # JUST CHURNED   \n",
    "    elif (month.iloc[x,1] > 0)&(month.iloc[x,2] > 0)&(month.iloc[x,3] > 0)&(month.iloc[x,4] > 0)&(month.iloc[x,5] > 0)&(month.iloc[x,6] > 0)&(month.iloc[x,7] > 0)&(month.iloc[x,8] > 0)&(month.iloc[x,9] > 0)&(month.iloc[x,10] > 0)&(month.iloc[x,11] > 0)&(month.iloc[x,12] == 0):\n",
    "        val = 2  # ALREADY CHURNED \n",
    "    elif (month.iloc[x,12] == 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0):\n",
    "        val = 2  # ALREADY CHURNED\n",
    "    elif (month.iloc[x,12] > 0)&(month.iloc[x,11] == 0)&(month.iloc[x,10] == 0)&(month.iloc[x,9] == 0):\n",
    "        val = 3  # NEW CUSTOMER (DON'T INVOLVE INTO MODEL -DROP- )\n",
    "    else:\n",
    "        val = 1 # REST COULD BE CONSIDERED ACTIVE\n",
    "    User_Status.append(val)\n",
    "User_Status = pd.DataFrame(User_Status,columns = {\"User_Status\"})\n",
    "# Integrate user definitions with pre generated time series dataset \n",
    "ts3_11 = pd.concat([month11,User_Status],axis=1)\n",
    "print(ts3_11.groupby('User_Status')['2019-12'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE A CONSOLIDATED DATAFRAME (ENTIRE 11 MONTH CHURNED USERS + THE MOST RECENT MONTH FULL DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namesList = ['USER_ID','1','2','3','4','5','6','7','8','9','10','11','12','13','Recency','Frequency','User_Status']\n",
    "# CREATE A CONSOLIDATED DATAFRAME (ENTIRE 11 MONTH CHURNED USERS + THE MOST RECENT MONTH FULL DATA)\n",
    "ts3_1_minority = ts3_1[ts3_1.User_Status==0];ts3_1_minority.columns = namesList\n",
    "ts3_2_minority = ts3_2[ts3_2.User_Status==0];ts3_2_minority.columns = namesList\n",
    "ts3_3_minority = ts3_3[ts3_3.User_Status==0];ts3_3_minority.columns = namesList\n",
    "ts3_4_minority = ts3_4[ts3_4.User_Status==0];ts3_4_minority.columns = namesList\n",
    "ts3_5_minority = ts3_5[ts3_5.User_Status==0];ts3_5_minority.columns = namesList\n",
    "ts3_6_minority = ts3_6[ts3_6.User_Status==0];ts3_6_minority.columns = namesList\n",
    "ts3_7_minority = ts3_7[ts3_7.User_Status==0];ts3_7_minority.columns = namesList\n",
    "ts3_8_minority = ts3_8[ts3_8.User_Status==0];ts3_8_minority.columns = namesList\n",
    "ts3_9_minority = ts3_9[ts3_9.User_Status==0];ts3_9_minority.columns = namesList\n",
    "ts3_10_minority = ts3_10[ts3_10.User_Status==0];ts3_10_minority.columns = namesList\n",
    "ts3_11_minority = ts3_11[ts3_11.User_Status==0];ts3_11_minority.columns = namesList\n",
    "ts3_11_majority = ts3_11[ts3_11.User_Status==1];ts3_11_majority.columns = namesList\n",
    "consolidation_list = [ts3_1_minority,ts3_2_minority,ts3_3_minority,ts3_4_minority,ts3_5_minority,ts3_6_minority,ts3_7_minority,ts3_8_minority,ts3_9_minority,ts3_10_minority,ts3_11_minority,ts3_11_majority]\n",
    "df = pd.concat(consolidation_list,axis=0)\n",
    "df = df.drop_duplicates(['USER_ID'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete previously created dummy datasets\n",
    "del ts3_1 ; del ts3_2 ; del ts3_3 ; del ts3_4 ; del ts3_5 ; del ts3_6 ; del ts3_7 ; del ts3_8 ; del ts3_9 ; del ts3_10 ;del ts3_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete previously created dummy datasets\n",
    "del month1;del month2;del month3;del month4;del month5;del month6;del month7;del month8;del month9;del month10;del month11 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a brief look on recency and frequency features on churn vs active users\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.distplot(df[df.User_Status == 0].Recency,hist=False)\n",
    "sns.distplot(df[df.User_Status == 1].Recency,hist=False)\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.distplot(df[df.User_Status == 0].Frequency,hist=False)\n",
    "sns.distplot(df[df.User_Status == 1].Frequency,hist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW FEATURE CREATION TO INCREASE ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# New Feature  - Median of last 6 months\n",
    "df['L6_Month_Median'] = df.iloc[:,7:13].median(axis=1)\n",
    "# New Feature  - Minimum of last 3 months\n",
    "df['L3_Month_Min'] = df.iloc[:,10:13].min(axis=1)\n",
    "# New Feature  - Standard Deviation of last 6 months\n",
    "df['L6_Month_STD'] = df.iloc[:,7:13].std(axis=1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIME SERIES PREDICTION TO PREDICT EACH USER'S FUTURE RECEIPT UPLOAD WITH EXPONENTIAL SMOOTHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform data by melting month columns into one column \n",
    "df_ts = df.loc[:,'USER_ID':'13']\n",
    "df_ts = df_ts.melt(id_vars = 'USER_ID',var_name='Month',value_name='Receipt Uploaded')\n",
    "df_ts.Month = df_ts.Month.astype(int)\n",
    "df_ts.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data by spreading User-ID's as seperate columns to make them ready for time series prediction with a loop for each\n",
    "df_ts = df.loc[:,'USER_ID':'13']\n",
    "df_ts = df_ts.melt(id_vars = 'USER_ID',var_name='Month',value_name='Receipt Uploaded')\n",
    "df_ts = df_ts.pivot(index='Month',columns = 'USER_ID', values = 'Receipt Uploaded').reset_index()\n",
    "df_ts.Month = pd.to_numeric(df_ts.Month)\n",
    "df_ts = df_ts.set_index('Month').sort_values(by='Month')\n",
    "df_ts.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        \n",
    "                            ######## EXPONENTIAL SMOOTHING PREDICTION LOOP FOR EACH USER ########\n",
    "SES_12M = []\n",
    "SES_3M = []\n",
    "\n",
    "upper = len(df_ts.columns)\n",
    "for x in range(0,upper):\n",
    "\n",
    "    # Sımple Exponentıal Smoothıng Predictions for 12 - 6 - 3  Month Periods\n",
    "    \n",
    "    # Prediction based on past 12 months\n",
    "    df_1  = df_ts.iloc[0:12,x]\n",
    "    ses_12 = SimpleExpSmoothing(df_1).fit()\n",
    "    ses_12_output = ses_12.forecast(1)\n",
    "    SES_12M.append(ses_12_output)\n",
    "    \n",
    "    del df_1;del ses_12; del ses_12_output # delete dummy datasets\n",
    "\n",
    "    \n",
    "    # Prediction based on past 3 months\n",
    "    df_5  = df_ts.iloc[9:12,x]\n",
    "    ses_3 = SimpleExpSmoothing(df_5).fit()\n",
    "    ses_3_output = ses_3.forecast(1)\n",
    "    SES_3M.append(ses_3_output)\n",
    "    \n",
    "    del df_5;del ses_3; del ses_3_output # delete dummy datasets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K Convert prediction results into a dataframe format\n",
    "ses_12 = pd.DataFrame(SES_12M).rename(columns={12: \"Prediction_SES_12M\"}).reset_index().iloc[:,1]\n",
    "ses_3 = pd.DataFrame(SES_3M).rename(columns={3: \"Prediction_SES_3M\"}).reset_index().iloc[:,1]\n",
    "\n",
    "# Combine each different prediction into one dataset\n",
    "df_columns = pd.DataFrame(df_ts.columns)\n",
    "list1 = [df_columns,ses_12,ses_3]\n",
    "data = pd.concat(list1,axis=1)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine existing datafarme with prediction results and create the ultimate prediction feature\n",
    "cols = ['USER_ID',\"12\",'13','User_Status','L6_Month_Median','L3_Month_Min','L6_Month_STD','Recency','Frequency']\n",
    "df_combined = df[cols].merge(data,left_on='USER_ID', right_on='USER_ID') \n",
    "df_combined = df_combined.rename(columns={'12': \"Previous_Month\",'13': \"Predicted_Month\"})\n",
    "list1 = ['Prediction_SES_3M','Prediction_SES_12M'] \n",
    "# Generate the ultimate prediction column by averaging 3 different exponential smoothing prediction results\n",
    "df_combined['Actual_Month_Prediction'] =  df_combined[list].mean(1)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMBINE CREATED TIME SERIES DATASET WITH COMPANY X DEMOGRAPHICS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K \"Birth Year ID\" to \"Age\" Conversion in order to utilize this feature in the model\n",
    "demographics = pd.read_csv('demographics.csv')\n",
    "def age_calculator(x):\n",
    "    return (2020 - x['BIRTH_YEAR_ID'])\n",
    "demographics['Age'] = demographics.apply(age_calculator,axis=1)\n",
    "demographics.head(2)\n",
    "\n",
    "#K Merge Demographics and Time-Series Datasets\n",
    "drop_cols = ['BIRTH_YEAR_ID','FIRST_TRANSACTION_DT','FIRST_STATIC_START_DATE','LAST_STATIC_END_DATE']\n",
    "holds = ['Actual_Month_Prediction','USER_ID','Previous_Month','User_Status','L6_Month_STD','L6_Month_Median','Recency','Frequency']\n",
    "final_df = demographics.merge(df_combined[holds],how='right',left_on='USER_ID', right_on='USER_ID').drop(drop_cols,axis=1)\n",
    "final_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE NEW FEATURES (PERIODICITY & DISCIPLINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert User ID and Basket ID into string format\n",
    "purchases['USER_ID']=purchases['USER_ID'].apply(str)\n",
    "purchases['BASKET_ID']=purchases['BASKET_ID'].apply(str)\n",
    "# Calculate unique transactions for each user\n",
    "df_periodicity=pd.DataFrame(purchases.groupby(['TRANSACTION_DT','USER_ID'])['BASKET_ID'].nunique().reset_index())\n",
    "# Create dummy features to be able to calculate Periodicity and Discipline Features (Previous Transaction & Days between transactions) \n",
    "df_periodicity['PREVIOUS_TRANS']=df_periodicity.groupby(['USER_ID'])['TRANSACTION_DT'].shift()\n",
    "df_periodicity['DAYS_BW_TRANS']=df_periodicity['TRANSACTION_DT']-df_periodicity['PREVIOUS_TRANS']\n",
    "df_periodicity['DAYS_BW_TRANS']=df_periodicity['DAYS_BW_TRANS'].apply(lambda x: x.days)\n",
    "\n",
    "# Use created features to distill a new dataframe with each User in seperate columns\n",
    "list1 = ['TRANSACTION_DT','USER_ID','DAYS_BW_TRANS']\n",
    "df_prd =  df_periodicity[list1].dropna()\n",
    "df_prd = df_prd.pivot(index='TRANSACTION_DT',columns='USER_ID',values='DAYS_BW_TRANS').reset_index().sort_values(by='TRANSACTION_DT',ascending=False)\n",
    "\n",
    "# Create a for loop for each user to calculate periodicity and discipline features\n",
    "periodicity = []\n",
    "discipline = []\n",
    "upper = len(df_prd.columns)\n",
    "for x in range(1,upper):\n",
    "    \n",
    "    df_1  = df_prd.iloc[:,x]\n",
    "    result = df_1.dropna().iloc[0:50].mean() # last 50 transaction\n",
    "    periodicity.append(result)\n",
    "    dis = df_1.dropna().iloc[0:50].std() # last 50 transaction\n",
    "    discipline.append(dis)\n",
    "\n",
    "# Create a new dataset with calculated features\n",
    "periodicity = pd.DataFrame(periodicity,columns=['periodicity'])\n",
    "discipline = pd.DataFrame(discipline,columns=['discipline'])\n",
    "users  = pd.DataFrame(df_prd.drop('TRANSACTION_DT',axis=1).columns.tolist(),columns=['USER_ID'])\n",
    "list1 = [users,periodicity,discipline]\n",
    "new_features = pd.concat(list1,axis=1)\n",
    "\n",
    "# Integrate new features into existing final dataset\n",
    "final_df['USER_ID']=final_df['USER_ID'].apply(str)\n",
    "final_df_v2 = final_df.merge(new_features,left_on='USER_ID', right_on='USER_ID')\n",
    "final_df_v2.discipline = final_df_v2.discipline * -1\n",
    "final_df_v2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete dummy datasets\n",
    "del df_periodicity;del df_prd; del final_df;del periodicity;del discipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE NEW FEATURES (RISKY CUSTOMERS & DUMMY USER STATUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D graph to explore features impact on User's activeness or churn potential\n",
    "fig = px.scatter_3d(final_df_v2, x='Actual_Month_Prediction', y='periodicity', z='discipline',\n",
    "              color='User_Status')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new Feature \"RISKY CUSTOMERS\" based on explorations above\n",
    "def risky(x):\n",
    "    if (x['Actual_Month_Prediction'] < 20) & (x['periodicity'] >3) & (x['discipline'] < -10):\n",
    "        val=1\n",
    "    else:\n",
    "        val=0\n",
    "    return val\n",
    "final_df_v2['Risky_Customers'] = final_df_v2.apply(risky,axis=1)\n",
    "\n",
    "#Create new Feature (Prev_Pred_Change) Difference between Predicted month and Previous Month to show trend\n",
    "final_df_v2['Prev_Median_Change'] =  final_df_v2.Previous_Month - final_df_v2.L6_Month_Median\n",
    "\n",
    "print(final_df_v2.Risky_Customers.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplotting of several features to observe their impact on User's activeness or churn potential\n",
    "sns.scatterplot('Actual_Month_Prediction','Recency',hue='User_Status',data=final_df_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new Feature \"dummy_user_status\" based on explorations above\n",
    "def dummy_user_status(x):\n",
    "    if (x['Actual_Month_Prediction'] < 0):\n",
    "        val='Churn'\n",
    "    elif (x['Recency'] > 25):\n",
    "        val='Churn'\n",
    "    elif (x['discipline'] < -20):\n",
    "        val='Churn'\n",
    "    elif (x['Actual_Month_Prediction'] > 15) & (x['Recency'] > 20):\n",
    "        val='Churn'\n",
    "    elif (x['Actual_Month_Prediction'] > 10) & (x['Recency'] > 25):\n",
    "        val='Churn'\n",
    "    elif (x['Actual_Month_Prediction'] > 30) & (x['Recency'] < 4):\n",
    "        val='Active'\n",
    "    else:\n",
    "        val='Unclear'\n",
    "    return val\n",
    "final_df_v2['dummy_user_status'] = final_df_v2.apply(dummy_user_status,axis=1)\n",
    "final_df_v2.groupby('dummy_user_status').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Feature -  User Channel Based Transaction Percentages \n",
    "# Create a new feature indicating user's channel preference\n",
    "list1 = ['USER_ID','CHANNEL_ID'];channel_trend = purchases[list1];channel_trend['N'] = 1\n",
    "channel_trend = pd.DataFrame(pd.pivot_table(channel_trend, values = 'N', index=['USER_ID'], \n",
    "                columns = ['CHANNEL_ID'],aggfunc='count')).fillna(0)\n",
    "channel_trend['Total_Channel_ Count'] = channel_trend.sum(1)\n",
    "channel_trend = channel_trend.div( channel_trend.iloc[:,-1], axis=0).drop('Total_Channel_ Count',axis=1).reset_index()\n",
    "channel_trend['USER_ID']=channel_trend['USER_ID'].apply(str)\n",
    "list1= ['USER_ID','User_Status']\n",
    "\n",
    "# Since there are tons of channels for the sake of simplicity elimination implemented based on their correlation to dependent variable\n",
    "channel_trend = final_df_v2[list1].merge(channel_trend,left_on='USER_ID', right_on='USER_ID') \n",
    "corr = pd.DataFrame(channel_trend.corr().User_Status).sort_values(by='User_Status')\n",
    "channel_features = corr[(corr.User_Status > 0.03)|(corr.User_Status < -0.03)].reset_index()['index'].tolist()\n",
    "channel_features.remove('User_Status');channel_features.append('USER_ID')\n",
    "final_df_v3 = final_df_v2.merge(channel_trend[channel_features],left_on='USER_ID', right_on='USER_ID') \n",
    "final_df_v3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Feature -  User Retail ID Based Transaction Percentages \n",
    "# Create a new feature indicating user's retailer preference\n",
    "list1 = ['USER_ID','RETAILER_ID'];retail_trend = purchases[list1];retail_trend['N'] = 1\n",
    "retail_trend = pd.DataFrame(pd.pivot_table(retail_trend, values = 'N', index=['USER_ID'], \n",
    "                columns = ['RETAILER_ID'],aggfunc='count')).fillna(0)\n",
    "\n",
    "#Eliminating small scale retailers and manipulating this dataframe to be compatible to a merge with main dataframe\n",
    "retail_filter = pd.DataFrame(retail_trend.sum(axis=0)).reset_index()\n",
    "retail_filter = retail_filter[retail_filter[0] > 50000][\"RETAILER_ID\"].tolist()\n",
    "retail_filter.append('USER_ID')\n",
    "retail_trend['Total_Retail_Count'] = retail_trend.sum(1)\n",
    "retail_trend = retail_trend.div( retail_trend.iloc[:,-1], axis=0).drop('Total_Retail_Count',axis=1).reset_index()\n",
    "retail_trend['USER_ID']=retail_trend['USER_ID'].apply(str)\n",
    "\n",
    "# Integrate recently generated feature to main dataframe\n",
    "list1= ['USER_ID','User_Status']\n",
    "retail_trend = final_df_v3[list1].merge(retail_trend[retail_filter],left_on='USER_ID', right_on='USER_ID') \n",
    "corr = pd.DataFrame(retail_trend.corr().User_Status).sort_values(by='User_Status')\n",
    "retail_features = corr[(corr.User_Status > 0.03)|(corr.User_Status < -0.03)].reset_index()['index'].tolist()\n",
    "retail_features.append('USER_ID') ; retail_features.remove('User_Status')\n",
    "final_df_v3 = final_df_v3.merge(retail_trend[retail_features],left_on='USER_ID', right_on='USER_ID') \n",
    "final_df_v3.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OUTLIER REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a scatterplot to explore major features' and their impact on the dependent variable\n",
    "plt.figure(figsize=(18,7))\n",
    "sns.stripplot(x='Recency',y='Actual_Month_Prediction',hue='Risky_Customers',data=final_df_v3[final_df_v3.User_Status == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier removal conducted with trial & error based on the highest model performance\n",
    "Churn_Data = final_df_v3[final_df_v3.User_Status == 0]\n",
    "Churn_Data_Outliers = Churn_Data[(Churn_Data.Previous_Month > 25) & (Churn_Data.Recency > 13)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the shapes of before/after datasets to see how many rows are eliminated\n",
    "print(final_df_v3.shape)\n",
    "list = [final_df_v3,Churn_Data_Outliers]\n",
    "final_df_v4 = pd.concat(list).drop_duplicates(keep=False)\n",
    "print(final_df_v4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age (No Major difference between actives and churns, so I am not going to segment this feature)\n",
    "sns.distplot(final_df_v4[final_df_v4['User_Status'] == 0].Age,hist=False)\n",
    "sns.distplot(final_df_v4[final_df_v4['User_Status'] == 1].Age,hist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTION WITH FINALIZED DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K Generate Dummy Variables for Categorical Features\n",
    "dummies = ['dummy_user_status','STATE_ID','RURAL_CODE','CENSUS_DIVISION_NAME','EMPLOYMENT_ID','GENDER_ID','EDUCATION_ID','ETHNICITY_ID','HH_SIZE_ID','INCOME_NEW_ID','RURAL_CODE']\n",
    "final_df_v5 = pd.DataFrame(pd.get_dummies(final_df_v4,columns=dummies,drop_first=True))\n",
    "final_df_v5.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing Model Data \n",
    "final_df_v5.to_csv('Model_Data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start making predictions with prominent ML models\n",
    "##### RANDOM FOREST #######\n",
    "drop = ['User_Status']\n",
    "X = final_df_v5.drop(drop,axis=1)\n",
    "y = final_df_v5['User_Status']\n",
    "y = np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=25)\n",
    "\n",
    "# 200 trees\n",
    "rfc = RandomForestClassifier(n_estimators=200)\n",
    "rfc.fit(X_train,y_train)\n",
    "X_train.shape\n",
    "\n",
    "# Prediction on Test Dataset\n",
    "rf_pred = rfc.predict(X_test)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REGULARIZATION TO ELIMINATE REDUNDANT VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization for Feature Importance\n",
    "drops = ['User_Status']\n",
    "X = final_df_v5.drop(drops,axis=1)\n",
    "y = final_df_v5['User_Status']\n",
    "y = np.array(y)\n",
    "# Used l1 penalty with 0.005 (trial & error)\n",
    "lsvc = LinearSVC(C=0.005, penalty=\"l1\", dual=False).fit(X, y) \n",
    "regularized_model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = regularized_model.transform(X)\n",
    "X_regularized = pd.DataFrame(X_new)\n",
    "Regularized_Features = X.columns[(regularized_model.get_support())]\n",
    "X_regularized = X[Regularized_Features]\n",
    "X_regularized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-predict with the eliminated features\n",
    "##### RANDOM FOREST #######\n",
    "X = final_df_v5[Regularized_Features]\n",
    "y = final_df_v5['User_Status']\n",
    "y = np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=25)\n",
    "\n",
    "# 200 trees\n",
    "rfc = RandomForestClassifier(n_estimators=200,class_weight=None)\n",
    "rfc.fit(X_train,y_train)\n",
    "X_train.shape\n",
    "# Prediction on Test Dataset\n",
    "rf_pred = rfc.predict(X_test)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Top Features to process clustering\n",
    "# Used random forest classifier package to see best performing features\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "df = final_df_v5[Regularized_Features]\n",
    "feat_importances = pd.Series(rfc.feature_importances_, index=df.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'Cluster' Feature based on top performing features calculated above\n",
    "top_features = ['discipline','Recency','Previous_Month','periodicity','Actual_Month_Prediction']\n",
    "df_cluster_t = final_df_v5[top_features]\n",
    "mms_t = MinMaxScaler()\n",
    "mms_t.fit(df_cluster_t)\n",
    "data_transformed_t = mms_t.transform(df_cluster_t)\n",
    "\n",
    "# Processing k-means clustering for up to 10 clusters\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(data_transformed_t)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    " \n",
    "# Plot the result to identify optimal K\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2 or 3 clusters seems bets,I choose to go ahead with 3 clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(data_transformed_t)\n",
    "Clusters = pd.DataFrame(kmeans.labels_)\n",
    "Clusters.columns = ['Clusters']\n",
    "\n",
    "# Integrate cluster columns to main model dataframe\n",
    "Regularized_Features_2 = Regularized_Features.tolist() ; Regularized_Features_2.append('USER_ID'); Regularized_Features_2.append('User_Status')\n",
    "Regularized_Features_2.remove('RURAL_CODE_Urban')\n",
    "final_df_v6 = pd.concat([final_df_v5[Regularized_Features_2],Clusters],axis=1,join='inner')\n",
    "dummies = ['Clusters']\n",
    "final_df_v6 = pd.DataFrame(pd.get_dummies(final_df_v6,columns=dummies,drop_first=True))\n",
    "final_df_v6.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREDICT WITH SELECTED FEATURES AND CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-predict with eliminated features and added clusters\n",
    "##### RANDOM FOREST #######\n",
    "drops = ['USER_ID','User_Status'] \n",
    "X = final_df_v6[Regularized_Features_2].drop(drops,axis=1)\n",
    "y = final_df_v6['User_Status']\n",
    "y = np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=25)\n",
    "\n",
    "# 200 trees\n",
    "rfc = RandomForestClassifier(n_estimators=200,class_weight=None)\n",
    "rfc.fit(X_train,y_train)\n",
    "X_train.shape\n",
    "# Prediction on Test Dataset\n",
    "rf_pred = rfc.predict(X_test)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with other ML algorithms\n",
    "### XGBOOST ###\n",
    "# Paramater tuning based trial and error results\n",
    "model1 = xgb.XGBClassifier(max_depth=9,booster='gbtree',learning_rate=0.065)\n",
    "X_train = np.array(X_train);y_train = np.array(y_train);X_test = np.array(X_test);y_test = np.array(y_test)\n",
    "train_model1 = model1.fit(X_train,y_train)\n",
    "xgb_pred = train_model1.predict(X_test)\n",
    "print(classification_report(y_test,xgb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOWN SAMPLING MAJORITY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the active user class is predominant in the dataset, downsampling applied in order to balance these two classes\n",
    "\n",
    "####### RANDOM FOREST REGULARIZED & CLUSTERED & BALANCED ######\n",
    "drops = ['USER_ID','User_Status'] \n",
    "X = final_df_v6[Regularized_Features_2].drop(drops,axis=1)\n",
    "y = final_df_v6['User_Status']\n",
    "y = np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=25)\n",
    "\n",
    "# optimal ratio is found by trials\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "ratio = {0:4402,1:12000}\n",
    "rus = RandomUnderSampler(sampling_strategy=ratio)\n",
    "X_train = np.array(X_train)\n",
    "X_train, y_train = rus.fit_sample(X_train, y_train)\n",
    "\n",
    "# 200 trees\n",
    "rfc = RandomForestClassifier(n_estimators=50,class_weight=None)\n",
    "rfc.fit(X_train,y_train)\n",
    "print(X_train.shape)\n",
    "\n",
    "# Prediction on Test Dataset\n",
    "rf_pred = rfc.predict(X_test)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for a later calculation for ShinyApp at the bottom\n",
    "\n",
    "y_test_df = pd.DataFrame(y_test).rename(columns={0: \"User_Status\"})\n",
    "X_test_df = X_test.reset_index()\n",
    "test_df = pd.concat([X_test_df,y_test_df],axis=1).drop('index',axis=1)\n",
    "test_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with other ML algorithms\n",
    "### XGBOOST ###\n",
    "model1 = xgb.XGBClassifier(max_depth=9,booster='gbtree',learning_rate=0.065)\n",
    "X_train = np.array(X_train);y_train = np.array(y_train);X_test = np.array(X_test);y_test = np.array(y_test)\n",
    "train_model1 = model1.fit(X_train,y_train)\n",
    "xgb_pred = train_model1.predict(X_test)\n",
    "print(classification_report(y_test,xgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with other ML algorithms\n",
    "### lOGISTIC REGRESSION ###\n",
    "log = LogisticRegression(solver='liblinear')\n",
    "log.fit(X_train,y_train)\n",
    "\n",
    "# Prediction on Test Dataset\n",
    "log_pred = log.predict(X_test)\n",
    "print(classification_report(y_test,log_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed Model of Random Forest and XGBoost\n",
    "mixed_pred = (rf_pred +xgb_pred) / 2\n",
    "mixed_pred = mixed_pred.astype('uint8')\n",
    "print(classification_report(y_test,mixed_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network Model \n",
    "\n",
    "X_train_nn = np.array(X_train);y_train_nn = np.array(y_train);X_test_nn = np.array(X_test);y_test_nn = np.array(y_test)\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train), y_train)                                                                           \n",
    "scaler = MinMaxScaler();X_train_nn = scaler.fit_transform(X_train);X_test_nn = scaler.transform(X_test)\n",
    "\n",
    "# Patience is set to 25 and dropout is determined as 0.1\n",
    "early_stop = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 25)\n",
    "model = Sequential()\n",
    "model.add(Dense(X_train.shape[1], activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss='binary_crossentropy',metrics=['accuracy'],learning_rate=0.0001)\n",
    "model.fit(X_train,y_train,epochs = 1000, validation_data = (X_test_nn, y_test_nn), callbacks=[early_stop],verbose = 0,class_weight=None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prediction on Test Dataset\n",
    "# Cutoff rate 0.52 yields the optimal results for precision and recall\n",
    "DL_pred = model.predict(X_test)\n",
    "nn_pred = (DL_pred > 0.52)\n",
    "print(classification_report(y_test,nn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# Calculated exact F1 Score\n",
    "f1_score(y_test, nn_pred, average=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted prediction results to uint format\n",
    "nn_pred = np.reshape(nn_pred,(7943,)).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mixed Model 2 (XGBoost and Neural Network Models)\n",
    "mixed = (xgb_pred +nn_pred) / 2\n",
    "mixed = mixed.astype('uint8')\n",
    "print(classification_report(y_test,mixed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HEREAFTER PART IS FOR SHINYAPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame(DL_pred).rename(columns={0: \"Model Prediction\"})\n",
    "list_2 = [test_df,prediction]\n",
    "assessment = pd.concat(list_2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(assessment[assessment.User_Status == 0]['Model Prediction'],hist=False)\n",
    "sns.distplot(assessment[assessment.User_Status == 1]['Model Prediction'],hist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = [1,2,3,4,5]\n",
    "assessment['binned'] =  pd.cut(assessment['Model Prediction'], bins=5, labels=labels)\n",
    "assessment.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn = pd.pivot_table(assessment, values='Model Prediction', index=['binned'],columns=['User_Status'], aggfunc='count').reset_index()\n",
    "df_churn['Churn Risk'] =df_churn[0]/(df_churn[0] + df_churn[1]) ; df_churn['Churn Risk'] =(df_churn['Churn Risk'].round(2) * 100).round(0)\n",
    "df_churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def description(x):\n",
    "    if x['binned'] == 1:\n",
    "        val = 'High Churn Risk ~95%'\n",
    "    elif x['binned'] == 2:\n",
    "        val = 'High-Moderate Churn Risk ~60%'\n",
    "    elif x['binned'] == 3:\n",
    "        val = 'Moderate Churn Risk ~40%'\n",
    "    elif x['binned'] == 4:\n",
    "        val = 'Low-Moderate Churn Risk ~15%'\n",
    "    else:\n",
    "        val = 'Low Churn Risk ~2%'\n",
    "    return val\n",
    "assessment['Description'] = assessment.apply(description,axis=1)\n",
    "assessment.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 = ['Recency','Frequency','periodicity','discipline','binned','Description','Model Prediction']\n",
    "assessment[list_1].to_csv('Assessment.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
